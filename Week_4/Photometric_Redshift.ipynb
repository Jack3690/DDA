{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Photometric_Redshift.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNhYwJ5SOGpQhERwdwiI44m"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JmDZGfucV2LU","colab_type":"text"},"source":["In this activity, we're going to use decision trees to determine the redshifts of galaxies from their photometric colours. We'll use galaxies where accurate spectroscopic redshifts have been calculated as our gold standard. We will learn how to assess the accuracy of the decision trees predictions and have a look at validation of our model.\n","\n","This activity is based on the scikit-learn example on Photometric Redshifts of Galaxies.<br>\n","http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/tutorial/astronomy/regression.html"]},{"cell_type":"code","metadata":{"id":"IH9vWegjWCc6","colab_type":"code","colab":{}},"source":["from google.colab import files\n","uploaded = files.upload()      #upload sdss_galaxy_colors.npy\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bWeqp9TXYFLc","colab_type":"code","colab":{}},"source":["data = np.load('sdss_galaxy_colors.npy')\n","print(data[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"em7F6N_9YPlH","colab_type":"text"},"source":["This is a SDSS galaxy dataset with flux in different filters and some other info including it's spectroscopic redshift<br>\n"," u &emsp;&emsp;&emsp;&emsp; g&emsp;&emsp;&emsp;&emsp;r&emsp;&emsp;&emsp;&emsp;i&emsp;&emsp;&emsp;&emsp;z &emsp;&emsp;&emsp;&emsp;Spec Class&emsp;redshift&emsp;&emsp;redshift_err <br>\n"," 19.84132 19.52656 19.46946 19.17955 19.10763&emsp; b'QSO'&emsp;&emsp; 0.539301 &emsp;  6.543622e-05<br>\n"," 19.86318 18.66298 17.84272 17.38978 17.14313&emsp; b'GALAXY' 0.1645703&emsp; 1.186625e-05<br>\n"," .<br>\n"," .<br>\n"," ."]},{"cell_type":"markdown","metadata":{"id":"T78zKWz-iDu9","colab_type":"text"},"source":["We are gonna use the five SDSS filters to define colours, which will be features for our decision tree.\n","The data[0] corresponds to the first row of the table above. Individual named columns can be accessed like this:"]},{"cell_type":"code","metadata":{"id":"xiIFOz0DkiwD","colab_type":"code","colab":{}},"source":["import numpy as np\n","data = np.load('sdss_galaxy_colors.npy')\n","print(data['u'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwZ71EnB2nVs","colab_type":"text"},"source":["# **Coding Assignment**"]},{"cell_type":"code","metadata":{"id":"ihcio9g3Waqd","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def get_features_targets(data):\n","\n","  dim=np.shape(data)                        # dim has the dimensions of the data table\n","  features=np.zeros((dim[0],4),float)       # Creates an array with as many rows as the sdss table and 4 coloums\n","  target=np.zeros((dim[0]))\n","  # fill the features and target array\n","  features[:,0]=data['u'] - data['g'] #Hint\n","  \n","\n","\n","\n","  \n","  \n","  return features,target\n","\n","\n","if __name__ == \"__main__\":\n","  # load the data\n","  data = np.load('sdss_galaxy_colors.npy')\n","    \n","  # call our function \n","  features, targets = get_features_targets(data)\n","    \n","  # print the shape of the returned arrays\n","  print(features[:2])\n","  print(targets[:2])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cxHbGzuHk4iQ","colab_type":"text"},"source":["We are now going to use our features and targets to train a decision tree and then make a prediction. We are going to use the DecisionTreeRegressor class from the sklearn.tree module."]},{"cell_type":"code","metadata":{"id":"r3HOjZiwkznL","colab_type":"code","colab":{}},"source":["import numpy as np\n","from sklearn.tree import DecisionTreeRegressor\n","\n","# copy in your get_features_targets function here\n","\n","\n","\n","\n","\n","\n","# load the data and generate the features and targets\n","data = np.load('sdss_galaxy_colors.npy')\n","features, targets = get_features_targets(data)\n","  \n","# initialize model\n","\n","# train the model\n","\n","# make predictions using the same features\n","\n","# print out the first 4 predicted redshifts\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fyOZ_qullZu3","colab_type":"text"},"source":["So we trained a decision tree! Great...but how do we know if the tree is actually any good at predicting redshifts?\n","\n","In regression we compare the predictions generated by our model with the actual values to test how well our model is performing. The difference between the predicted values and actual values (sometimes referred to as residuals) can tell us a lot about where our model is performing well and where it is not.\n","\n","While there are a few different ways to characterise these differences, in this tutorial we will use the median of the differences between our predicted and actual values. This is given by:\n","\n","$median$ difference = median (||$Y_i,pred$  - $Y_i,calc$||)\n","\n","First we'll seperate our data into two sets: training set and testing set.\n","\n","We'll train our Decision tree regresson using the training set and test it on the testing set and calculate median difference"]},{"cell_type":"code","metadata":{"id":"nRYxFQuWFneQ","colab_type":"code","colab":{}},"source":["import numpy as np\n","data = np.load('sdss_galaxy_colors.npy')\n","# split data into training data and testing data\n","\n","\n","\n","\n","# Splitting training data into features and targets\n","Train_features, Train_targets = get_features_targets(Train_data)\n","# Splitting testing data into features and targets\n","Test_features, Test_targets = get_features_targets(Test_data)\n","\n","\n","\n","# initialize model\n","\n","# train the model using training set\n","\n","# make predictions using the  Test_features\n","\n","\n","# Write a function median_diff to find median difference between Test_targets and predictions\n","def median_diff(Test,predict):\n"," # Hint: use numpy abs for finding modulus  \n","\n","\n","\n","median_difference=median_diff(Test_targets,predictions)\n","\n","print(\"Median difference :\",median_difference)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eEGWoF7oG4Nl","colab_type":"text"},"source":["The method we used to validate our model is known as hold-out validation. Hold out validation splits the data in two, one set to test with and the other to train with. Hold out validation is the most basic form of validation.\n","\n","#**k-fold cross validation**\n","In order to be more certain of our models accuracy we should use **k -fold** cross validation. k -fold validation works in a similar way to hold-out except that we split the data into k subsets. We train and test the model k times, recording the accuracy each time. Each time we use a different combination of **k-1** subsets to train the model and the final kth subset to test. We take the average of the k accuracy measurements to be the overall accuracy of the the model."]}]}